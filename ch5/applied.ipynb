{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         sklearn_sm,\n",
    "                         summarize)\n",
    "from ISLP import confusion_table\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     cross_validate,\n",
    "                                     KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "> In Chapter $4$, we used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(a)** Fit a logistic regression model that uses `income` and `balance` to predict `default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>711.555020</td>\n",
       "      <td>52992.378914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>757.962918</td>\n",
       "      <td>19660.721768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>845.411989</td>\n",
       "      <td>58636.156984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1569.009053</td>\n",
       "      <td>36669.112365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>200.922183</td>\n",
       "      <td>16862.952321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     default student      balance        income\n",
       "0         No      No   729.526495  44361.625074\n",
       "1         No     Yes   817.180407  12106.134700\n",
       "2         No      No  1073.549164  31767.138947\n",
       "3         No      No   529.250605  35704.493935\n",
       "4         No      No   785.655883  38463.495879\n",
       "...      ...     ...          ...           ...\n",
       "9995      No      No   711.555020  52992.378914\n",
       "9996      No      No   757.962918  19660.721768\n",
       "9997      No      No   845.411989  58636.156984\n",
       "9998      No      No  1569.009053  36669.112365\n",
       "9999      No     Yes   200.922183  16862.952321\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Default_df = load_data('Default')\n",
    "Default_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = MS(['income', 'balance'])\n",
    "X = design.fit_transform(Default_df)\n",
    "y = Default_df.default == 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_predictors(summary_df, threshold=0.05):\n",
    "    \"\"\"Get rows with significant p-value from DataFrame\"\"\"\n",
    "    return summary_df.loc[summary_df[summary_df.columns[-1]] <= threshold]\n",
    "\n",
    "def get_accuracy(confusion_mat, decimals=4):\n",
    "    return (np.trace(confusion_mat) / \n",
    "            np.sum(confusion_mat.to_numpy())).round(decimals=decimals)\n",
    "\n",
    "def get_error_rate(confusion_mat, decimals=4):\n",
    "    return (1 - get_accuracy(confusion_mat)).round(decimals=decimals)\n",
    "\n",
    "def get_prediction(fitted_model, X, threshold=0.5):\n",
    "    \"\"\"Get predictions for `fitted_model`\"\"\"\n",
    "    probs = fitted_model.predict(X)\n",
    "    predictions = np.full(X.shape[0], False)\n",
    "    predictions[probs >= threshold] = True\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.0263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>9629</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>38</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      False  True \n",
       "Predicted              \n",
       "False       9629    225\n",
       "True          38    108"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = sm.GLM(y, X,\n",
    "                  family=sm.families.Binomial()).fit()\n",
    "confusion_mat = confusion_table(get_prediction(logit, X), y)\n",
    "print(f'Error rate: {get_error_rate(confusion_mat)}')\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(b)** Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
    "> <ol type=\"i\">\n",
    "> <li>Split the sample set into a training set and a validation set.</li>\n",
    "> <li>Fit a multiple logistic regression model using only the training observations.</li>\n",
    "> <li>Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the `default` category if the posterior probability is greater than $0.5$.</li>\n",
    "> <li>Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.</li>\n",
    "> </ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_set(X, y, test_size=0.3, pred_threshold=0.5, rd_seed=None):\n",
    "    \"\"\"Validation set approach for logistic regression\"\"\"\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, \n",
    "                                                        random_state=rd_seed)\n",
    "    lr_model = sm.GLM(y_train, X_train, family=sm.families.Binomial()).fit()\n",
    "    predictions = get_prediction(lr_model, X_valid, threshold=pred_threshold)\n",
    "    confusion_mat = confusion_table(predictions, y_valid)\n",
    "    return confusion_mat, predictions, (X_train, X_valid, y_train, y_valid), lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 0.7\n",
      "Estimated test error rate: 0.026\n"
     ]
    }
   ],
   "source": [
    "confusion_mat, _, _, _ = validation_set(X, y, test_size=1-0.7)\n",
    "print(f'Training size: {0.7}')\n",
    "print(f'Estimated test error rate: {get_error_rate(confusion_mat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(c)** Repeat the process in $(b)$ three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 0\n",
      "Estimated test error rate: 0.0273\n",
      "Random seed: 1\n",
      "Estimated test error rate: 0.0247\n",
      "Random seed: 2\n",
      "Estimated test error rate: 0.0237\n"
     ]
    }
   ],
   "source": [
    "for seed in range(3):\n",
    "    confusion_mat, _, _, _ = validation_set(X, y, rd_seed=seed)\n",
    "    print(f'Random seed: {seed}')\n",
    "    print(f'Estimated test error rate: {get_error_rate(confusion_mat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We can see that the division of the data set into validation set (controlled by the seed) can indeed affect the result of the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(d)** Now consider a logistic regression model that predicts the probability of `default` using `income`, `balance`, and a dummy variable for `student`, Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 0\n",
      "Error rate: 0.0292\n",
      "Random seed: 1\n",
      "Error rate: 0.0262\n",
      "Random seed: 2\n",
      "Error rate: 0.0254\n"
     ]
    }
   ],
   "source": [
    "# Model including `student`\n",
    "design = MS(['income', 'balance', 'student'])\n",
    "X = design.fit_transform(Default_df)\n",
    "for seed in range(3):\n",
    "    confusion_mat, _, _, _ = validation_set(X, y, test_size=0.5, rd_seed=seed)\n",
    "    print(f'Random seed: {seed}')\n",
    "    print(f'Error rate: {get_error_rate(confusion_mat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Comparing the model from part $(c)$ and the new model's result, including `student` appears to have no improvement. In fact, the average of the three validation sets appears to favor the model without `student` as a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "> We continue to consider the use of a logistic regression model to predict the probability of `default` using `income` and `balance` on the `Default` data set. In particular, we will now compute estimates for the standard errors of the `income` and `balance` logistic regression coefficients in two different ways: $(1)$ using the bootstrap, $(2)$ using the standard formula for computing the standard error in the `sm.GLM()` function. Do not forget to set a random seed before beginning your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(a)** Using the `summarize()` and `sm.GLM()` functions, determine the estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model that uses both predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-11.540500</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>-26.544</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.174</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef   std err       z  P>|z|\n",
       "intercept -11.540500  0.435000 -26.544    0.0\n",
       "income      0.000021  0.000005   4.174    0.0\n",
       "balance     0.005600  0.000000  24.835    0.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design = MS(['income', 'balance'])\n",
    "X = design.fit_transform(Default_df)\n",
    "logit = sm.GLM(y, X, family=sm.families.Binomial()).fit()\n",
    "summarize(logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(b)** Write a function, `boot_fn()`, that takes as input the `Default` data set as well as an index of the observations, and that outputs the coefficient estimates for `income` and `balance` in the multiple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(dataset, idx):\n",
    "    \"\"\"Find estimates for `income` and `balance`\"\"\"\n",
    "    design = MS(['income', 'balance'])\n",
    "    X = design.fit_transform(Default_df).iloc[idx]\n",
    "    y = (dataset.default == 'Yes').iloc[idx]\n",
    "    logit = sm.GLM(y, X, family=sm.families.Binomial()).fit()\n",
    "    summary = summarize(logit)\n",
    "    income_se = summary['coef']['income']\n",
    "    balance_se = summary['coef']['balance']\n",
    "    return income_se, balance_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(c)** Following the bootstrap example in the lab, use your `boot_fn()` function to estimate the standard errors in the logistic regression coefficients for `income` and `balance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_SE(func, dataset, n=None, B=1000, seed=0):\n",
    "    \"\"\"Perform bootstrap to find SE for `income` and `balance`\n",
    "    \n",
    "    Note: SE = sqrt( Var ) = sqrt( E[X^2] - (E[X])^2 )\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : function\n",
    "        Function to find SE of `income` and `balance` at given indices\n",
    "    dataset\n",
    "        Dataset to perform bootstrap\n",
    "    n : int\n",
    "        Size of each bootstrap\n",
    "    B : int\n",
    "        Number of bootstraps\n",
    "    seed : int\n",
    "        Seed for random number generator\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_income, second_income = 0, 0\n",
    "    first_balance, second_balance = 0, 0\n",
    "    n = n or dataset.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx =  rng.choice(dataset.index, n, replace=True)\n",
    "        income_se, balance_se = func(dataset, idx)\n",
    "        first_income += income_se\n",
    "        second_income += income_se**2\n",
    "        first_balance += balance_se\n",
    "        second_balance += balance_se**2\n",
    "    return (np.sqrt(second_income / B - (first_income / B)**2), \n",
    "            np.sqrt(second_balance / B - (first_balance / B)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.767215650997462e-06, 0.0002334009425870289)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_SE(boot_fn, Default_df, n=Default_df.shape[0], B=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(d)** Comment on the estimated standard errors obtained using the `sm.OLS()` function and using the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Comparing to results from $(a)$, the _standard error_ for `income` matches well. However, `balance` is off. <br><br>\n",
    "_Note_: I suspect that I have done something wrong with fitting model that uses the full data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "> In Sections $5.1.2$ and $5.1.3$, we saw that the `cross_validate()` function can be used in order to compute the $LOOCV$ test error estimate. Alternatively, one could compute those quantities using just `sm.GLM()` and the `predict()` method of the fitted model within a for loop. You will now take this approach in order to compute the $LOOCV$ error for a simple logistic regression model on the `Weekly` data set. Recall that in the context of classification problems, the $LOOCV$ error is given in $(5.4)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(a)** Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "      <td>3.514</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>1.178</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>2010</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-2.173</td>\n",
       "      <td>3.599</td>\n",
       "      <td>0.015</td>\n",
       "      <td>3.205160</td>\n",
       "      <td>2.969</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>2010</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-2.173</td>\n",
       "      <td>3.599</td>\n",
       "      <td>4.242568</td>\n",
       "      <td>1.281</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>2010</td>\n",
       "      <td>1.281</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-2.173</td>\n",
       "      <td>4.835082</td>\n",
       "      <td>0.283</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.281</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>4.454044</td>\n",
       "      <td>1.034</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>2010</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.281</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>2.707105</td>\n",
       "      <td>0.069</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1089 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n",
       "0     1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n",
       "1     1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down\n",
       "2     1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up\n",
       "3     1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up\n",
       "4     1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up\n",
       "...    ...    ...    ...    ...    ...    ...       ...    ...       ...\n",
       "1084  2010 -0.861  0.043 -2.173  3.599  0.015  3.205160  2.969        Up\n",
       "1085  2010  2.969 -0.861  0.043 -2.173  3.599  4.242568  1.281        Up\n",
       "1086  2010  1.281  2.969 -0.861  0.043 -2.173  4.835082  0.283        Up\n",
       "1087  2010  0.283  1.281  2.969 -0.861  0.043  4.454044  1.034        Up\n",
       "1088  2010  1.034  0.283  1.281  2.969 -0.861  2.707105  0.069        Up\n",
       "\n",
       "[1089 rows x 9 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weekly_df = load_data('Weekly')\n",
    "Weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design = MS(['Lag1', 'Lag2'])\n",
    "X = design.fit_transform(Weekly_df)\n",
    "y = Weekly_df.Direction == 'Up'\n",
    "logit = sm.GLM(y, X, family=sm.families.Binomial()).fit()\n",
    "\n",
    "predictions = get_prediction(logit, X)\n",
    "confusion_mat = confusion_table(predictions, y)\n",
    "get_error_rate(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(b)** Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2` _using all but the first observation_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.443"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design = MS(['Lag1', 'Lag2'])\n",
    "X = design.fit_transform(Weekly_df)[1:]\n",
    "y = (Weekly_df.Direction == 'Up')[1:]\n",
    "logit = sm.GLM(y, X, family=sm.families.Binomial()).fit()\n",
    "\n",
    "predictions = get_prediction(logit, X)\n",
    "confusion_mat = confusion_table(predictions, y)\n",
    "get_error_rate(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(c)** Use the model from $(b)$ to predict the direction of the first observation. You can do this by predicting that the first observation will go up if $P(\\text{Direction = \"Up\"|Lag1, Lag1}) \\geq 0.5$. Was this observation correctly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Up\n",
      "True y: Down\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/8hy6tr792sb5d5lgkbz_wpp40000gn/T/ipykernel_1869/3346578582.py:1: FutureWarning: Series.bool is now deprecated and will be removed in future version of pandas\n",
      "  print(\"Prediction: Up\" if (logit.predict(X.iloc[0]) > 0.5).bool()\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: Up\" if (logit.predict(X.iloc[0]) > 0.5).bool() \n",
    "      else \"Prediction: Down\")\n",
    "print(\"True y: Up\" if y.iloc[0] else \"True y: Down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logit.predict(X.iloc[0]) > 0.5) != y.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The prediction was incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(d)** Write a for loop from $i = 1$ to $i = n$, where $n$ is the number of observations in the data set, that performs each of the following steps:\n",
    "> <ol type='i'>\n",
    "> <li> Fit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.</li>\n",
    "> <li>Compute the posterior probability of the market moving up for the ith observation.</li>\n",
    "> <li>Use the posterior probability for the ith observation in order to predict whether or not the market moves up.</li>\n",
    "> <li>Determine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.</li>\n",
    "> </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None    599\n",
       "dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design = MS(['Lag1', 'Lag2'])\n",
    "X = design.fit_transform(Weekly_df)\n",
    "y = (Weekly_df.Direction == 'Up')\n",
    "\n",
    "num_correct = 0\n",
    "for i in range(X.shape[0]):\n",
    "    design = MS(['Lag1', 'Lag2'])\n",
    "    X = design.fit_transform(Weekly_df)\n",
    "    y = (Weekly_df.Direction == 'Up')\n",
    "    X_train = X.drop([i])\n",
    "    y_train = y.drop([i])\n",
    "    logit = sm.GLM(y_train, X_train, family=sm.families.Binomial()).fit()\n",
    "    num_correct += (logit.predict(X.iloc[i]) > 0.5) == y.iloc[i]\n",
    "num_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(e)** Take the average of the $n$ numbers obtained in $(d)iv$ in order to obtain the $LOOCV$ estimate for the test error. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated test error: 0.4499540863177227\n"
     ]
    }
   ],
   "source": [
    "print(f'Estimated test error: {1 - (num_correct/X.shape[0]).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The test error rate is less than $50\\%$ meaning the model is doing better than random guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "> We will now perform cross-validation on a simulated data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(a)** Generate a simulated data set as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "x = rng.normal(size=100)\n",
    "y = x - 2 * x**2 + rng.normal(size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** There are $100$ observations so $n=100$ <br>\n",
    "$y$ is model only by $x$ so $p=1$ <br><br>\n",
    "The full model is:\n",
    "$$ y = x - 2x^2 + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(b)** Create a scatterplot of $X$ against $Y$. Comment on what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = subplots()\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** It follows the expected negative quadratic relationship between $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(c)** Set a random seed, and then compute the LOOCV error that result from fitting the following four models using least squares:\n",
    "> - $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
    "> - $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$\n",
    "> - $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$\n",
    "> - $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon$\n",
    "> <!-- end list -->\n",
    "> Note you may find it helpful to use the `data.frame()` function to create a single data set containing both $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model includes up to x^1: 6.633029839181984\n",
      "Model includes up to x^2: 1.1229368563419688\n",
      "Model includes up to x^3: 1.3017965489358871\n",
      "Model includes up to x^4: 1.332394269417932\n"
     ]
    }
   ],
   "source": [
    "cv_errors = np.zeros(4)\n",
    "model = sklearn_sm(sm.OLS)\n",
    "loocv = KFold(n_splits=x.shape[0],\n",
    "              shuffle=True,\n",
    "              random_state=0)\n",
    "# Notice that I use `neg_MSE` here, hence the negation when updating `cv_errors`\n",
    "for i, d in enumerate(range(1, 5)):\n",
    "    X = np.power.outer(x, np.arange(d+1))\n",
    "    M_CV = cross_validate(model,\n",
    "                            X,\n",
    "                            y,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv=loocv)\n",
    "    cv_errors[i] = - np.mean(M_CV['test_score'])\n",
    "\n",
    "for i, model in enumerate(cv_errors):\n",
    "    print(f'Model includes up to x^{1+i}: {cv_errors[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(d)** Repeat $(c)$ using another random seed, and report your results. Are you results the same as what you got in $(c)$? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model includes up to x^1: 6.633029839181985\n",
      "Model includes up to x^2: 1.1229368563419686\n",
      "Model includes up to x^3: 1.3017965489358871\n",
      "Model includes up to x^4: 1.332394269417932\n"
     ]
    }
   ],
   "source": [
    "model = sklearn_sm(sm.OLS)\n",
    "loocv = KFold(n_splits=x.shape[0],\n",
    "              shuffle=True,\n",
    "              random_state=2)\n",
    "for i, d in enumerate(range(1, 5)):\n",
    "    X = np.power.outer(x, np.arange(d+1))\n",
    "    M_CV = cross_validate(model,\n",
    "                            X,\n",
    "                            y,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv=loocv)\n",
    "    cv_errors[i] = - np.mean(M_CV['test_score'])\n",
    "\n",
    "for i, model in enumerate(cv_errors):\n",
    "    print(f'Model includes up to x^{1+i}: {cv_errors[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The result is the same as that of $(C)$. This is understandable since LOOCV uses all observation at least once as validation set. This will always happens regardless of the order that they are in.<br>\n",
    "Finding the mean of the same summation (with positions of elements inside the summation changed) does not affect the result. <br><br>\n",
    "_Note_: Some of the last decimals is showing difference between the two results. This is due to __[floating point error](https://www.geeksforgeeks.org/floating-point-error-in-python/)__ caused when the order of the summation is changed and not due to the above argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(e)** Which of the models in $(c)$ had the smallest LOOCV error? Is this what you expected? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The quadratic model has the best MSE. This is as expected considering that our true model shares that same relationship. <br>\n",
    "More complex models suffer from over-fitting (variance) while the linear model suffers (a lot) from its wrong assumption on the true model (bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(f)** Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in $(c)$ using least squares. Do these results agree with the conclusion drawn based on the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model includes up to x^1\n",
      "         coef  std err      t  P>|t|\n",
      "const -1.4650    0.247 -5.937    0.0\n",
      "x1     1.9494    0.289  6.752    0.0\n",
      "Model includes up to x^2\n",
      "         coef  std err       t  P>|t|\n",
      "const -0.0728    0.119  -0.611  0.543\n",
      "x1     0.9663    0.126   7.647  0.000\n",
      "x2    -2.0047    0.091 -22.072  0.000\n",
      "Model includes up to x^3\n",
      "         coef  std err       t  P>|t|\n",
      "const -0.0572    0.120  -0.477  0.635\n",
      "x1     1.1146    0.187   5.945  0.000\n",
      "x2    -2.0471    0.099 -20.673  0.000\n",
      "x3    -0.0643    0.060  -1.070  0.287\n",
      "Model includes up to x^4\n",
      "         coef  std err       t  P>|t|\n",
      "const  0.1008    0.136   0.743  0.460\n",
      "x1     0.9050    0.205   4.423  0.000\n",
      "x2    -2.5059    0.221 -11.336  0.000\n",
      "x3     0.0338    0.073   0.466  0.642\n",
      "x4     0.1042    0.045   2.309  0.023\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(range(1, 5)):\n",
    "    X = np.power.outer(x, np.arange(d+1))\n",
    "    design = sm.OLS(y, X)\n",
    "    model  = design.fit()\n",
    "    print(f'Model includes up to x^{1+i}')\n",
    "    print(summarize(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** All models agree on the significance of $x$ and $x^2$. This agrees with our conclusion from using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "> We will now consider the `Boston` housing data set, from the `ISLP` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(a)** Based on this data set, provide an estimate for the population mean of `medv`. Call this estimate $\\hat{\\mu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
       "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
       "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
       "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
       "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
       "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
       "\n",
       "     ptratio  lstat  medv  \n",
       "0       15.3   4.98  24.0  \n",
       "1       17.8   9.14  21.6  \n",
       "2       17.8   4.03  34.7  \n",
       "3       18.7   2.94  33.4  \n",
       "4       18.7   5.33  36.2  \n",
       "..       ...    ...   ...  \n",
       "501     21.0   9.67  22.4  \n",
       "502     21.0   9.08  20.6  \n",
       "503     21.0   5.64  23.9  \n",
       "504     21.0   6.48  22.0  \n",
       "505     21.0   7.88  11.9  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston_df = load_data('Boston')\n",
    "Boston_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110677"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston_df.medv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(b)** Provide an estimate of the standard error of $\\hat{\\mu}$. Interpret this result. <br> *HInt: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4088611474975351"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston_df.medv.std() / np.sqrt(Boston_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Interpretation: the _standard error_ $\\hat{\\mu}$ of `medv` indicates how much \n",
    "deviation from the true $\\mu$ (of `medv`) we would see if we take a sample from the population and find mean of `medv` of that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(c)** Now estimate the standard error of $\\hat{\\mu}$ using the bootstrap. How does this compare to your answer from $(b)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_mean_fn(dataset, idx):\n",
    "    \"\"\"Find SE for mean of `medv` at given indices\"\"\"\n",
    "    return dataset.medv.iloc[idx].mean()\n",
    "\n",
    "def boot_mean_SE(func, dataset, n=None, B=100, seed=0):\n",
    "    \"\"\"Perform bootstrap to find SE for mean of `medv` in Boston dataset\n",
    "    \n",
    "    Note: SE = sqrt( Var ) = sqrt( E[X^2] - (E[X])^2 )\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : function\n",
    "        Function to find mean of `medv` at given indices\n",
    "    dataset\n",
    "        Dataset to perform bootstrap\n",
    "    n : int\n",
    "        Size of each bootstrap\n",
    "    B : int\n",
    "        Number of bootstraps\n",
    "    seed : int\n",
    "        Seed for random number generator\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_ , second_ = 0, 0\n",
    "    for i in range(B):\n",
    "        idx = rng.choice(Boston_df.shape[0], size=n)\n",
    "        se = func(Boston_df, idx)\n",
    "        first_ += se\n",
    "        second_ += se**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2), first_ / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41110616476936196"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE, mu_hat = boot_mean_SE(boot_mean_fn, Boston_df, n=Boston_df.shape[0], B=10000, seed=0)\n",
    "SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The estimated standard error is very close to the previous result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(d)** Based on your bootstrap estimate from $(c)$, provide a $95\\%$ confidence interval for the mean of `medv`. Compare it to the results obtained by using `Boston['medv'].std()` and the two standard error rule $(3.9)$. <br>\n",
    "> *Hint: You can approximate a $95\\%$ confidence interval using the formula $[\\hat{\\mu} - 2SE(\\hat{\\mu}), \\hat{\\mu} + 2SE(\\hat{\\mu})]$*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21.71284761117278, 23.35727227025023]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mu_hat was is average mu from bootstrap\n",
    "[mu_hat - 2 * SE, mu_hat + 2 * SE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.138598149351036, 40.92701449887032]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mean = Boston_df['medv'].mean()\n",
    "true_std = Boston_df['medv'].std()\n",
    "[true_mean - 2 * true_std, true_mean + 2 * true_std]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** This result is as expected. `true_mean` $\\sim$ `mu_hat`. `SE` is the _standard deviation_ of _estimated mean_, meaning it's much closer to the _mean_ itself. On the other hand, `true_std` is finding the _standard deviation_ of the observations to the _mean_, which is much greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(e)** Based on this dataset, provide an estimate, $\\hat{\\mu}_{med}$, for the median value of `medv` in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.2"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston_df.medv.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(f)** We now would like to estimate the standard error of $\\hat{\\mu}_{med}$. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37687862766090385"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can utilize the same function to find SE of median using Bootstrap\n",
    "def boot_med_fn(dataset, idx):\n",
    "    \"\"\"Find SE for median of `medv` at given indices\"\"\"\n",
    "    return dataset.medv.iloc[idx].median()\n",
    "\n",
    "SE, _ = boot_mean_SE(boot_med_fn, Boston_df, n=Boston_df.shape[0], B=10000, seed=0)\n",
    "SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The _standard error_ for the _median_ is smaller (better) than that of the _mean_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(g)** Based on this data set, provide an estimate for the tenth percentile of `medv` in Boston census tracts. call this quantity $\\hat{\\mu}_{0.1}$. (You can use the `np.percentile()` function.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.75"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(Boston_df['medv'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **(h)** Use the bootstrap to estimate the standard error of $\\hat{\\mu}_{0.1}$. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035641412764847"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boot_10_fn(dataset, idx):\n",
    "    \"\"\"Find SE for 10 percentile of `medv` at given indices\"\"\"\n",
    "    return np.percentile(dataset.medv.iloc[idx], 10)\n",
    "\n",
    "SE, _ = boot_mean_SE(boot_10_fn, Boston_df, n=Boston_df.shape[0], B=10000, seed=0)\n",
    "SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** For a relatively small data set (`Boston` contains roughly $500$ observations), the obtained _standard error_ at $0.5$ can be consider small. This means that the estimated $10$-percentile is quite accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E-Search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
