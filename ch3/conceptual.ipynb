{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_o$: &nbsp; $\\beta_{TV} = \\beta_{radio} = \\beta_{newspaper} = 0$ \\\n",
    "$H_a$: &nbsp; $\\exists \\beta_j \\neq 0$ for $j \\in $ {`TV`, `radio`, `newspaper`}\n",
    "\n",
    "Conclusion:\n",
    "- Assuming that significant level $\\alpha$ is 5% or $\\alpha$ = 0.05. $p$ value for `TV` and `radio` is less than $\\alpha$ to reject the null hypothesis that the relationship between these predictors and the response `unit sold`\n",
    "- On the other hand, $p$ value for `newspaper` is greater than $\\alpha$ and we failed to reject the null hypothesis that there is no relationship with `unit sold` and this predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `KNN-classifier` classifies a new observation into a class based on the plurality class of the `k` closest neighbors.\n",
    "- `KNN-regression` predicts the value of a new observation by the mean value of the `k` closest neighbors: \n",
    "$\\hat{f_{x_o}} = \\frac{1}{k}\\sum_{x_i \\in N_o} y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**  Standard model\n",
    "$$ y = \\hat{\\beta_0} + \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 + \\hat{\\beta_3} X_3 + \\hat{\\beta_4} (X_1 X_2) + \\hat{\\beta_5} (X_1 X_3) $$\n",
    "substitute in known values for $X_3 \\in \\{0, 1\\}$, $\\hat{\\beta_5} = -10$ and $ \\hat{\\beta_3} = 35$\n",
    "$$ y_{college} = \\hat{\\beta_0} + \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 + 35 * 1 + \\hat{\\beta_4} (X_1 X_2) - 10 (X_1 * 1) $$\n",
    "$$ y_{high-school} = \\hat{\\beta_0} + \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 + 35 * 0 + \\hat{\\beta_4} (X_1 X_2) - 10 (X_1 * 0) $$\n",
    "we can eliminate shared terms between the two models and simplify for an easier comparison\n",
    "$$ y_{college} = 35 - 10 X_1 $$\n",
    "$$ y_{high-school} = 0 $$\n",
    "discrepancy in pay is the difference between the two model\n",
    "$$ \n",
    "\\begin{align}\n",
    "y_{diff}\n",
    " & = y_{college} - y_{high-school}  \\\\\n",
    " & = 35 - 10 X_1\n",
    "\\end{align}\n",
    "$$\n",
    "- *i.* Incorrect. Whether *high-school* or *college* degree earns more depends on the value of `GPA`. No conclusion can be made without information about this value.\n",
    "- *ii.* Incorrect. (similar reasoning as i.)\n",
    "- *iii.* Correct. Specifically, when `GPA` $ \\geq 3.5 \\Rightarrow y_{diff} \\leq 0 $\n",
    "- *iv.* Incorrect. When `GPA` is high enough, model suggests the opposite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** From the standard model from (a), plug in $X_1 = 4.0$, $X_2 = 110$, and $x_3 = 1$ $\\Rightarrow \\hat{y}=137.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** False, the strength of the interaction effect is independent of its coefficient estimated value. Whether or not there is an interaction effect should be determined through its Standard Error using $p$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Considering the bias-variance tradeoff and that the true model is linear, the cubic regression model has higher variance leading to over-fitting due to its high flexibility while not benefitting much, or at all, from possible lower bias. \\\n",
    "*(Can this also leads to problem due to high collinearity in the predictors?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Performance on testing dataset depends on how well model generalize. In this case, being closer to the true model, linear model would perform better resulting in a lower RSS for due to its lower bias (with inherent low variance from its inflexibility). Cubic model, on the other hand, suffers from high variance, even though bias might still be maintained low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We would expect the cubic regression to have lower RSS after finished training. When the true model in non-linear, cubic model benefits from its flexibility to obtain a lower variance. It also has lower bias compared to the linear model since there are less assumptions being made on the true model. This should hold true for the training data. Both models, however, should have their RSS decrease through training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** There is not enough information to tell. \\\n",
    "Their performances depend on the relationship between predictors and the response of the true model. If this relationship is nearly linear, (b) is correct, and the opposite holds true the more non-linear the it gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The notation has been adjusted slightly for better clarity.\n",
    "$\n",
    "\\begin{align}\n",
    "\\hat{y}_i \n",
    "  &= x_i \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i' = 1}^n x^2_{i'}} \\\\\n",
    "  &= x_i \\frac{\\sum_{i'=1}^nx_{i'}y_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} \\\\\n",
    "  &= \\frac{\\sum_{i'=1}^n x_i x_{i'}y_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} \\\\\n",
    "  &= \\sum_{i'=1}^n \\frac{ x_i x_{i'}y_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} \\\\\n",
    "  &= \\sum_{i'=1}^n \\frac{ x_i x_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} y_{i'}\n",
    "\\end{align}\n",
    "$\n",
    "Hence:\n",
    "$$\n",
    "a_{i'} = \\frac{x_i x_{i'}}{\\sum_{i''=1}^n x^2_{i''}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Substitute $\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}$ and $x = \\bar{x}$ into\n",
    "$\n",
    "\\begin{align}\n",
    "y \n",
    " & = \\hat{\\beta_0} + \\hat{\\beta_1}x \\\\\n",
    " & = \\bar{y} - \\hat{\\beta_1}\\bar{x} + \\hat{\\beta_1} \\bar{x} \\\\\n",
    " & = \\bar{y}\n",
    "\\end{align}\n",
    "$\n",
    "Least square line passes through the point ($\\bar{x}, \\bar{y}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We have:\n",
    "$$ R^2 = \\frac{\\textit{TSS} - \\textit{RSS}}{\\textit{TSS}} = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2 - \\sum_{i=1} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} $$\n",
    "$$ Cor(x,y) = \\frac{\\sum_i (x_i-\\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} $$\n",
    "given that $\\bar{x} = \\bar{y} = 0$\n",
    "$$ R^2 = \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n y_i^2} $$\n",
    "$$ Cor(x,y)^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_ix_i^2 \\sum_iy_i^2} $$\n",
    "note that\n",
    "$$ \\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} x_i $$\n",
    "where (from equation `3.4`)\n",
    "$$ \\hat{\\beta_1} = \\frac{\\sum_{j=1}^n (x_j - \\bar{x})(y_j - \\bar{y})}{\\sum_{j=1}^n (x_j - \\bar{x})^2} $$\n",
    "$$ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x} $$\n",
    "$$ \\Rightarrow \\hat{y_i} = x_i \\frac{\\sum_{j=1}^n x_j y_j}{\\sum_{j=1}^n x_j^2} $$\n",
    "Proof:\n",
    "$$\n",
    "\\begin{align}\n",
    "R^2\n",
    " & = \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n y_i^2} \\\\\n",
    " & = \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n (y_i - x_i \\frac{\\sum_{j=1}^n x_j y_j}{\\sum_{j=1}^n x_j^2})^2}{\\sum_{i=1}^n y_i^2} \\\\\n",
    " & = \\frac{2\\frac{\\sum_{j=1}^n x_j y_j}{\\sum_{j=1}^n x_j^2} \\sum_{i=1}^n x_i y_i - (\\frac{\\sum_{j=1}^n x_j y_j}{\\sum_{j=1}^n x_j^2})^2 \\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n y_i^2} \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "note that $\\sum_{i=1}^n x_i^2 = \\sum_{j=1}^n x_j^2$ since it is the same set of data (same for $y$), we can simplify by multiplying $\\sum_{j=1}^n x_j^2$ to both the numerator and denominator\n",
    "$$\n",
    "\\begin{align}\n",
    "R^2\n",
    " & = \\frac{2\\sum_{j=1}^n x_j y_j \\sum_{i=1}^n x_i y_i - (\\sum_{j=1}^n x_j y_j)^2}{\\sum_{j=1}^n x_j \\sum_{i=1}^n y_j} \\\\\n",
    " & = \\frac{(\\sum_ix_iy_i)^2}{\\sum_ix_i^2 \\sum_iy_i^2} = Cor(x,y)^2 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E-Search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
