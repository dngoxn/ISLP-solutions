{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.\n",
    "> Using a little bit of algebra, prove that $(4.2)$ is equivalent to $(4.3)$. In other words, the logistic function representation and logit representation for the logistic regression model are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** \n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} $$\n",
    "$$ \\Leftrightarrow \\qquad \\frac{1}{p(X)} = 1 + \\frac{1}{e^{\\beta_0 + \\beta_1 X}} $$\n",
    "$$ \\Leftrightarrow \\qquad \\frac{1}{p(X)} - 1 = \\frac{1}{e^{\\beta_0 + \\beta_1 X}} $$\n",
    "$$ \\Leftrightarrow \\qquad \n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.\n",
    "> It was stated in the text that classifying an observation to the class for which $(4.17)$ is largest is equivalent to classifying an observation to the class for which $(4.18)$ is largest. Prove that this is the case. In other words, under the assumption that the observations in the $k^{th}$ class are drawn from a $ N(\\mu_k, \\sigma^2) $ distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** From $(4.17)$:\n",
    "$$ p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi} \\sigma} exp(- \\frac{1}{2 \\sigma^2(x - \\mu_k)^2})}{\\sum_{l=1}^K {\\pi_l \\frac{1}{\\sqrt{2 \\pi} \\sigma} exp(- \\frac{1}{2 \\sigma^2} (x - \\mu_l)^2)}} $$\n",
    "Taking the log of both side:\n",
    "$$ log(p_k(x)) = log(numerator) - log(denominator) $$\n",
    "Notice that since we are maximizing for $ p_k(x) $, varying variable is related to each class $k$.<br> \n",
    "Knowing that $ log $ is a monotonic operation, we can remove any part of the equation that is not optimizable, or constants in particular. Specifically, we are maximizing variables related to each class $ k $ $ \\rightarrow $ possible to treat $x$ as constant. $\\sigma$ is shared between all $k$ classes $\\rightarrow$ treat as constant. <br><br>\n",
    "We find each part separately:\n",
    "- For each x, $ log(denominator) $ is always a constant. Only numerator is needed.\n",
    "- Numerator:\n",
    "$$\n",
    "\\begin{align}\n",
    "log(numerator)\n",
    "& = log(\\pi_k) + log(\\frac{1}{\\sqrt{2 \\pi} \\sigma}) - \\frac{x^2 - 2x \\mu_k + \\mu_k^2}{2 \\sigma^2} \\\\\n",
    "& = log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "> This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where $p = 1$; i.e. there is only one feature. <br> Suppose that we have $k$ classes, and that if an observation belongs to the $k^{th}$ class then $X$ comes from a one-dimensional normal distribution $ X \\sim N(\\mu_k, \\sigma_k^2) $. Recall that the density function for the one-dimensional normal distribution is given in $(4.16)$. Prove that in this case, the Bayes classifier is $not$ linear. Argue that it is in fact quadratic. <br> *Hint: For this problem, you should follow the arguments laid out in Section $4.4.1$, but without making the assumption that $ \\sigma_1^2 = ... = \\sigma_K^2 $.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We will follow the steps in $2$ with the exception that $\\sigma$ can no longer be treated as a constant. Each of $k$ classes now has their own $\\sigma_k$. However, the denominator is still a constant. Equation $(4.15)$ from the book shows this clearly. <br><br>\n",
    "We now have:\n",
    "$$\n",
    "log(\\pi_k \\frac{1}{\\sqrt{2 \\pi} \\sigma_k} exp(- \\frac{1}{2\\sigma_k^2} (x - \\mu_k))) = log(\\mu_k) + log(\\frac{1}{\\sqrt{2 \\pi} \\sigma_k}) - \\frac{x^2 - 2 x \\mu_k + \\mu_k^2}{2 \\sigma_k^2} \\\\\n",
    "$$\n",
    "With the last part of the equation in $quadratic$ form of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.\n",
    "> When the number of features $p$ is large, there tends to be a deterioration of $KNN$ and other $local$ approaches that perform prediction using only observations that are $near$ the test observations for which a prediction must be made. This phenomenon is known as the *curse of dimensionality*,  and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We will now investigate this curse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Suppose that we have a set of observations, each with measurements $ p = 1$ feature, $X$. We assume that X is uniformly (evenly) distributed on $[0, 1]$, Associated with each observation is a response value. Suppose that we wish to predict a test observation's response using only observations that are within $10\\%$ of the range of $X$ closest to that test observation. For instance, in order to predict response for a test observation with $X = 0.6$, we will use observations in the range $[0.55, 0.65]$. On average, what fraction of the available observations will we use to make the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** For test observations with values in the range $[0, 0.05]$, we actually have less than $10\\%$ available. The same applies to those in range $[0.95, 1]$. We only use, on average, $7.6\\%$ observations. The remaining range $[.05, 0.095]$ use $10\\%$ of the observations. <br><br>\n",
    "In total, we use $ 7.5\\% \\times (0.05 + 0.05) + 10\\% \\times 0.9 = 9.75\\% $ per test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Now suppose that we have a set of observations, each with measurements on $p = 2$ features, $X_1$, and $X_2$. We assume that $(X_1, X_2)$ are uniformly distributed on $ [0, 1] \\times [0, 1] $. we wish to predict a test observation's response using only observations that are within $10\\%$ of the range of $X_1$ *and* within $10\\%$ of the range of $X_2$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in the range $[0.55, 0.65]$ for $X_1$ and in the range $[0.3, 0.4]$ for $X_2$. On average, what fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Similar to part $(a)$, but now a 2-$dimensional$ case. Since $X_1$ and $X_2$ are independent, we can find that, on average, each test observation uses $ 0.975^2 = 9.50625 \\times 10^{-3} \\approx 0.95\\% $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) Now suppose tha twe have a set of observation on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from $0$ to $1$. We wish to predict a test observation's response using observations within the $10\\%$ of each feature;s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Applying the same logic, each the fraction of observations used is $ 0.975^{100}\\times 100\\% \\approx 0\\% $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">(d) Using your answers to parts $(a)$-$(c)$, argue that a drawback of KNN when $p$ is large is that there are very few training observations \"near\" any given test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We can observe that the fraction of available observations follow the function:\n",
    "$$ f(p) = (\\frac{1}{a})^p $$\n",
    "Where $a = \\text{number of neighbors } (> 0)$. With large $p$, this function approaches $0$. There is almost no available training observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (e) Now suppose that we wish to make a prediction for a test observation by creating a $p$-dimensional hypercube centered around the test observation that contains, on average, $10\\%$ of the training observations. For $p = 1, 2, \\text{and } 100$, what is the length of each side of the hypercube? Comment on your answer. <br><br> *Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When $p = 1$, a hypercube is simply a line segment, when $p = 2$ it is a square, and when $p = 100$ it is a $100$-dimensional cube.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We want to keep the number of neighbors to be $10\\%$ regardless of $p$.<br>\n",
    "For $p = 1$, line segment has length of $l = 0.1$ <br>\n",
    "For $p = 2$, square has area of $0.1$ so its length is $ l = 0.1^{1/2} = \\sqrt{0.1} \\approx 0.316$ <br>\n",
    "For arbitrary $p$, length of the hypercube is $ l = 0.1^{1/p} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.\n",
    "> We now examine the differences between $LDA$ and $QDA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) If the Bayes decision boundary is linear, do we expect $LDA$ or $QDA$ to perform better on the training set? On the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** $QDA$ is more flexible so it will perform better on the *training* set. However, $LDA$ is closer to the true boundary on unknown data. It will perform better on *testing* set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) If the Bayes decision boundary is non-linear, do we expect $LDA$ or $QDA$ to perform better on the training set? On the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Similar to $(a)$, $QDA$ will perform better on training set. However, the true boundary is now non-linear, on *testing* set, $QDA$ will perform better with higher flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (C) In general, as the sample size $n$ increases, do we expect the test prediction accuracy of $QDA$ relative to $LDA$ to improve, decline, or be unchanged? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** As $n$ grows, this will reduces the *variance* of both models. But $QDA$ also have the advantage of having a lower *bias* by having less assumptions on the model (or more flexible). The relative accuracy of $QDA$ relative to $LDA$ will improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using $QDA$ rather than $LDA$ because $QDA$ is flexible enough to model a linear decision boundary. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Generally, *False* because $QDA$ will likely to overfit to the data. But if the $n$ is large enough, it might be able to compensate for this. (It will not out-perform $LDA$ even if $n$ however since that also benefits $LDA$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.\n",
    "> Suppose we collect data for a group of students in a statistics class with variables $X_1 = \\text{hours studies}$, $X_2 = \\text{undergrad GPA}$, and $Y = \\text{receive an A}$. we fit a logistic regression and produce estimated coefficient, $\\hat{\\beta_0} = -6, \\hat{\\beta_1} = 0.05, \\hat{\\beta_2} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Estimate the probability that a student who studies for $40$ h and has an undergrad GPA of $3.5$ gets an A in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Plugging into equation $4.7$:\n",
    "$$\n",
    "p(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}\n",
    "$$\n",
    "Where $X_1 = 40$ and $X_2 = 3.5$ $\\Rightarrow p(X) = 0.377$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) How many hours would the student in part $(a)$ need to study to have a $50\\%$ chance of getting an A in the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}} $$\n",
    "$$ \\Leftrightarrow \\qquad -6 + 0.05X_1 + X_2 = log(\\frac{p(x)}{1 - p(x)}) $$\n",
    "$$ \\Leftrightarrow \\qquad -6 + 0.05X_1 + 3.5 = log(\\frac{0.5}{1 - 0.5}) $$\n",
    "$$ \\Leftrightarrow \\qquad X_1 = 50 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "> Suppose that we wish to predict whether a given stock will issue a dividend this year (\"Yes\" or \"No\") based on $X$, last year's percent profit. We examine a large number of companies and discover that the mean value for $X$ for companies that issued a dividend was $\\bar{X} = 10$, while the mean for those that didn't was $\\bar{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $\\sigma^2 = 36$. Finally, $80\\%$ of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage progit was $X = 4$ last year. <br><br> *Hint: Recall that the density function for a normal random variable is $f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x - \\mu)^2 / 2\\sigma^2}$. You will need to use Bayes' theorem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We will use a $LDA$ model since the given information follows its assumption. <br>\n",
    "We can find the class of the observation using the equation $4.17$ (Bayes' theorem was used here, refer to the book for more detail from equation $4.15$), or equivalently $4.18$ as proven by question $1$.\n",
    "$$ \\delta_k(x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k) $$\n",
    "Find the value of the discriminant function for each class, where $ \\mu_{Yes} = 10, \\mu_{No} = 0, \\sigma = 36, \\pi_{Yes} = 0.8, \\pi_{No} = 0.2 $\n",
    "- $ \\delta_{Yes} = 4 \\cdot \\frac{10}{36} - \\frac{10^2}{36^2} + log(0.8)$\n",
    "- $ \\delta_{No} = 4 \\cdot \\frac{0}{36} - \\frac{0}{36^2} + log(0.2) $\n",
    "<!-- end list -->\n",
    "$\\delta_{Yes}$ is larger so we choose $\\text{Yes}$ as the class for the new observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.\n",
    "> Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of $20\\%$ on the training data and $30\\%$ on the test data. Next we use $1$-nearest neighbors (i.e. $K = 1$) and get an average error rate (averaged over both test and training data sets) of $18\\%$. Based on these results, which method should we prefer to use for classification of new observations? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** KNN model using 1 closest neighbor is extremely is flexible. We can assume that it does very well on the training set, or error rate for training data set $\\approx 0\\%$. This assumption implies that its error rate on the testing ∂ata set $\\approx 36\\%$. We will pick the logistic model with lower error rate on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.\n",
    "> This problem has to do with $odds$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) On average, what fraction of people with an odds of $0.37$ of defaulting on their credit card payment will in fact default?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We have: <br>\n",
    "$ \\text{Odds} = p(X) / (1 - p(X)) = 0.37 $ <br>\n",
    "$ \\Leftrightarrow \\qquad p(X) \\approx 0.27 $ <br>\n",
    "Or 27 will default per 100 people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Suppose that an individual has $16\\%$ chance of defaulting on her credit card payment. What are the odds that she will default?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** $ p(X) = 0.16 / (1 - 0.16) = 4/21 \\approx 0.19 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.\n",
    "> Equation $4.32$ derived an expression for $log(\\frac{Pr(Y=k | X=x)}{Pr(Y=K | X=x)})$ in the setting where $p > 1$, so that the mean for the $k^{th}$ class, $\\mu_k$, is a $p$-dimensional vector, and the shared covariance $\\sum$ is a $p \\times p$ matrix. However, in the setting with $p = 1$, $(4.32)$ takes a simpler form, since the means $\\mu_1, …, \\mu_K$ and variance $\\sigma^2$ are scalars. In this simpler setting, repeat the calculation in $(4.32)$, and provide expressions for $a_k$ and $b_{kj}$ in terms of $\\pi_k, \\pi_K, \\mu_k, \\mu_K, \\text{ and}, \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log \\left(\\frac{Pr(Y=k | X=x)}{Pr(Y=K | X=x)} \\right)\n",
    "& = \\log \\left(\\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n",
    "& = \\log \\left(\\frac{\\pi_k exp(- \\frac{1}{2\\sigma^2} (x - \\mu_k)^2)}{\\pi_K exp(- \\frac{1}{2\\sigma^2} (x - \\mu_K)^2)} \\right) \\\\\n",
    "& = \\log \\left(\\frac{\\pi_k}{\\pi_K} \\right) - \\frac{1}{2\\sigma^2} (x - \\mu_k)^2 + \\frac{1}{2\\sigma^2} (x - \\mu_K)^2 \\\\\n",
    "& = \\log \\left(\\frac{\\pi_k}{\\pi_K} \\right) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) + \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_K + \\mu_K^2) \\\\\n",
    "& = \\log \\left(\\frac{\\pi_k}{\\pi_K} \\right) + \\frac{1}{2\\sigma^2} (-x^2 + 2x\\mu_k - \\mu_k^2 + x^2 - 2x\\mu_K + \\mu_K^2) \\\\\n",
    "& = \\log \\left(\\frac{\\pi_k}{\\pi_K} \\right) - \\frac{\\mu_k^2 - \\mu_K^2}{2\\sigma^2} + x\\frac{\\mu_k - \\mu_K}{\\sigma^2} \\\\\n",
    "& = a_k + b_k x\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Where\n",
    "$$\n",
    "\\begin{align*} \n",
    "& a_k = \\log \\left(\\frac{\\pi_k}{\\pi_K} \\right) - \\frac{\\mu_k^2 - \\mu_K^2}{2\\sigma^2} \\\\\n",
    "& b_k = \\frac{\\mu_k - \\mu_K}{\\sigma^2} \n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. \n",
    "> Work out the detailed forms of $a_k$, $b_k$, and $b_{kjl}$ in $(4.33)$. Your answer should involve $ \\pi_k, \\pi_K, \\mu_k, \\mu_K, \\sigma_k$, and $ \\sigma_K $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** *Note*, the derivation is quite tedious, but it's simply expanding the *Multivariate Gaussian density* function from equation $4.23$\n",
    "$$ \n",
    "\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) \n",
    " = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n",
    "$$\n",
    "Where\n",
    "$$ f_k(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma_k|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) \\right) $$\n",
    "$$ f_K(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma_K|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_K)^T \\Sigma_K^{-1} (x - \\mu_K) \\right) $$\n",
    "Hence\n",
    "$$\n",
    "\\begin{align*}\n",
    "& = \\log \\left( \\frac{\\pi_k \\frac{1}{(2\\pi)^{p/2} |\\Sigma_k|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) \\right)}{\\pi_K \\frac{1}{(2\\pi)^{p/2} |\\Sigma_K|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_K)^T \\Sigma_K^{-1} (x - \\mu_K) \\right)} \\right)  \\\\\n",
    "& = \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\n",
    " - \\left(\\log \\pi_K - \\frac{1}{2} \\log |\\Sigma_K| - \\frac{1}{2} (x - \\mu_K)^T \\Sigma_K^{-1} (x - \\mu_K)\\right) \\\\\n",
    "& = \\log \\frac{\\pi_k}{\\pi_K} - \\frac{1}{2} \\log \\frac{|\\Sigma_k|}{|\\Sigma_K|} - \\frac{1}{2} \\left[ (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) - (x - \\mu_K)^T \\Sigma_K^{-1} (x - \\mu_K) \\right] \\\\\n",
    "& = \\log \\frac{\\pi_k}{\\pi_K} - \\frac{1}{2} \\log \\frac{|\\Sigma_k|}{|\\Sigma_K|} - \\frac{1}{2} \\left[ x^T \\Sigma_k^{-1} x - 2x^T \\Sigma_k^{-1} \\mu_k + \\mu_k^T \\Sigma_k^{-1} \\mu_k - x^T \\Sigma_K^{-1} x + 2x^T \\Sigma_K^{-1} \\mu_K - \\mu_K^T \\Sigma_K^{-1} \\mu_K \\right] \\\\\n",
    "& = \\log \\frac{\\pi_k}{\\pi_K} - \\frac{1}{2} \\log \\frac{|\\Sigma_k|}{|\\Sigma_K|} - \\frac{1}{2} \\left[ x^T (\\Sigma_k^{-1} - \\Sigma_K^{-1}) x - 2x^T (\\Sigma_k^{-1} \\mu_k - \\Sigma_K^{-1} \\mu_K) + (\\mu_k^T \\Sigma_k^{-1} \\mu_k - \\mu_K^T \\Sigma_K^{-1} \\mu_K) \\right] \\\\\n",
    "& = a_k + \\sum_{j=1}^{p} b_{kj} x_j + \\sum_{j=1}^{p} \\sum_{l=1}^{p} b_{kjl} x_j x_l\n",
    "\\end{align*}\n",
    "$$\n",
    "With\n",
    "$$\n",
    "\\begin{align*}\n",
    "& a_k = \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k \\\\\n",
    "& b_{kj} = -(\\Sigma_k^{-1} \\mu_k)_j \\\\\n",
    "& b_{kjl} = -\\frac{1}{2} (\\Sigma_k^{-1})_{jl}\n",
    "\\end{align*}\n",
    "$$\n",
    "The summation is the expansion from vector form to non-vector to standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12.\n",
    "> Suppose that you wish to classify an observation $X \\in R$ into `apple` and `oranges`. You fit a logistic regression model and find that $$ \\hat{Pr}(Y = \\text{orange} | X = x) = \\frac{\\exp(\\hat{\\beta_0} + \\hat{\\beta_1} x)}{1 + \\exp(\\hat{\\beta_0} + \\hat{\\beta_1}x)} $$ Your friend fits a logistic regression model to the same data using the $softmax$ formulation in $(4.13)$, and finds that $$ \\hat{Pr}(Y = \\text{orange} | X = x) = \\frac{\\exp(\\hat{\\alpha}_{orange0} + \\hat{\\alpha}_{orange1}x)}{\\exp(\\hat{\\alpha}_{orange0} + \\hat{\\alpha}_{orange1}x) + \\exp(\\hat{\\alpha}_{apple0} + \\hat{\\alpha}_{apple1}x)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) What is the log odds of `orange` versus `apple` in your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** $Logit =  \\hat{\\beta_0} + \\hat{\\beta_1}x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) What is the log odds of `orange` versus `apple` in your friend's model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** $Logit = \\log \\left(\\frac{\\hat{Pr}(Y=\\text{orange} | X=x)}{\\hat{Pr}(Y=\\text{apple} | X=x)} \\right) = (\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) Suppose that in your model, $\\hat{\\beta_0} = 2$ and $\\hat{\\beta_1} = -1$. What are the coefficient estimates in your friend's model? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Assume that both models arrive at the same result, then:\n",
    "$$ \\hat{\\beta_0} + \\hat{\\beta_1}x = (\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x $$\n",
    "So \n",
    "$$ \\hat{\\beta_0} = \\hat\\alpha_{orange0} - \\hat\\alpha_{apple0} = 2 $$\n",
    "$$ \\hat{\\beta_1} = \\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -1 $$\n",
    "The exact value for each coefficient is unknown.\n",
    "<br><br>\n",
    "Another approach that I thought of was forcing equality on the equations for $\\hat{Pr}(Y=\\text{orange} |  X=x)$ for both models. This forces the coefficients in the *softmax* model for `orange` to follow exactly that of the *logistic* model. From that, I forced $\\hat{\\alpha_0} = \\hat{\\alpha_1} = 0$. However, this approach force the two models to have the exact structure, even in the $\\hat{Pr}()$ and not just the final $logit$. This might not be true at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (d) Now suppose that you and your friend fir the same two models in a different data set. This time, your friend gets the coefficient estimates $\\hat{\\alpha}_\\text{orange0} =1.2, \\hat{\\alpha}_\\text{orange1} = -2, \\hat{\\alpha}_\\text{apple0} = 3, \\hat{\\alpha}_\\text{apple1} = 0.6$. What are the coefficient estimates in your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We will make the same assumption, hence:\n",
    "$$ \\hat{\\beta_0} + \\hat{\\beta_1}x = (\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x $$\n",
    "So \n",
    "$$ \\hat{\\beta_0} = \\hat\\alpha_{orange0} - \\hat\\alpha_{apple0} = 1.2 - 3 = -1.8 $$\n",
    "$$ \\hat{\\beta_1} = \\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -2 -0.6 = -2.6 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (e) Finally, suppose you apply both models from $(d)$ to a data set with $2,000$ test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend's model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** We expect the results to be the same for both models since they are basically the same model. We can show they are equivalent model by setting all coefficients of the $K^{th}$ class to $0$, this result in the exact model for *Multinomial LR*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E-Search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
